import json
import re
from typing import List, Optional, Dict, Any, Tuple
from domain.entity.rag_memory import RAGMemory, MemoryType
from domain.interfaces.ai_client import AIClientInterface
from infrastructure.monitoring.logging import StructuredLogger


class RAGService:
    """Ð¡ÐµÑ€Ð²Ð¸Ñ Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¸ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ"""

    def __init__(self, ai_client: AIClientInterface):
        self.ai_client = ai_client
        self.logger = StructuredLogger("rag_service")

    async def extract_memories_from_message(self, user_id: int, message: str,
                                            conversation_context: List[Dict] = None) -> List[RAGMemory]:
        """Ð˜Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð²Ð°Ð¶Ð½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹ Ð¸Ð· ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ LLM"""

        # ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ñ„Ð°ÐºÑ‚Ð¾Ð²
        system_prompt = """
Ð¢Ñ‹ â€” Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð²Ñ‹Ð´ÐµÐ»ÑÐµÑ‚ Ð²Ð°Ð¶Ð½Ñ‹Ðµ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹ Ð¸Ð· ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.

Ð˜Ð—Ð’Ð›Ð•ÐšÐÐ™ Ð¢ÐžÐ›Ð¬ÐšÐž:
- Ð›Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸Ñ (Ð»ÑŽÐ±Ð»ÑŽ/Ð½Ðµ Ð»ÑŽÐ±Ð»ÑŽ, Ð½Ñ€Ð°Ð²Ð¸Ñ‚ÑÑ/Ð½Ðµ Ð½Ñ€Ð°Ð²Ð¸Ñ‚ÑÑ)
- Ð’Ð°Ð¶Ð½Ñ‹Ðµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ (Ð´Ð½Ð¸ Ñ€Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ, Ð³Ð¾Ð´Ð¾Ð²Ñ‰Ð¸Ð½Ñ‹, Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ñ)
- Ð›Ð¸Ñ‡Ð½Ñ‹Ðµ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸ (Ð°Ð»Ð»ÐµÑ€Ð³Ð¸Ð¸, Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ, Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡ÐºÐ¸)
- Ð”Ð¾Ð»Ð³Ð¾ÑÑ€Ð¾Ñ‡Ð½Ñ‹Ðµ Ð¿Ð»Ð°Ð½Ñ‹ Ð¸ Ñ†ÐµÐ»Ð¸
- Ð—Ð½Ð°Ñ‡Ð¸Ð¼Ñ‹Ðµ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ñ (ÑÐµÐ¼ÑŒÑ, Ð´Ñ€ÑƒÐ·ÑŒÑ)
- Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð½Ð°Ð²Ñ‹ÐºÐ¸ Ð¸ ÑƒÐ¼ÐµÐ½Ð¸Ñ

ÐÐ• Ð˜Ð—Ð’Ð›Ð•ÐšÐÐ™:
- ÐŸÐ¾Ð²ÑÐµÐ´Ð½ÐµÐ²Ð½Ñ‹Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ (ÑÐµÐ³Ð¾Ð´Ð½Ñ Ð¿Ð¾ÑˆÐµÐ» Ð² Ð¼Ð°Ð³Ð°Ð·Ð¸Ð½)
- Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ ÑÐ¼Ð¾Ñ†Ð¸Ð¸ (ÑÐµÐ³Ð¾Ð´Ð½Ñ Ð³Ñ€ÑƒÑÑ‚Ð½Ð¾/Ð²ÐµÑÐµÐ»Ð¾)
- ÐžÐ±Ñ‰Ð¸Ðµ Ñ€Ð°Ð·Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ñ Ð±ÐµÐ· ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð¸ÐºÐ¸
- ÐŸÐ¾Ð²ÐµÑ€Ñ…Ð½Ð¾ÑÑ‚Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¸ Ð¾ Ð¿Ð¾Ð³Ð¾Ð´Ðµ Ð¸ Ñ‚.Ð´.

Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð° - JSON:
{
    "memories": [
        {
            "type": "fact|preference|event|personal_detail",
            "content": "ÐšÑ€Ð°Ñ‚ÐºÐ°Ñ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ° Ñ„Ð°ÐºÑ‚Ð°",
            "importance": 0.8,
            "reason": "ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ ÑÑ‚Ð¾Ñ‚ Ñ„Ð°ÐºÑ‚ Ð²Ð°Ð¶ÐµÐ½"
        }
    ]
}

ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð¢ÐžÐ›Ð¬ÐšÐž JSON, Ð±ÐµÐ· Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ñ‚ÐµÐºÑÑ‚Ð¾Ð².
"""

        user_prompt = f"""
Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ: "{message}"

ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð° (Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ):
{self._format_conversation_context(conversation_context) if conversation_context else "ÐÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°"}

ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð¸ Ð¸Ð·Ð²Ð»ÐµÐºÐ¸ Ð²Ð°Ð¶Ð½Ñ‹Ðµ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹.
"""

        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            response = await self.ai_client.generate_response(
                messages,
                max_tokens=1000,
                temperature=0.1  # ÐÐ¸Ð·ÐºÐ°Ñ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð´Ð»Ñ ÐºÐ¾Ð½ÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð½Ð¾ÑÑ‚Ð¸
            )

            # ÐŸÐ°Ñ€ÑÐ¸Ð¼ JSON Ð¾Ñ‚Ð²ÐµÑ‚
            memories_data = self._parse_llm_response(response)

            memories = []
            for mem_data in memories_data.get('memories', []):
                memory = RAGMemory(
                    user_id=user_id,
                    memory_type=MemoryType(mem_data['type']),
                    content=mem_data['content'],
                    source_message=message,
                    importance_score=mem_data['importance'],
                    metadata={'extraction_reason': mem_data['reason']}
                )
                memories.append(memory)

            self.logger.info(
                f"Extracted {len(memories)} memories from user {user_id} message",
                extra={'user_id': user_id, 'message_length': len(message), 'memories_count': len(memories)}
            )

            return memories

        except Exception as e:
            self.logger.error(f"Error extracting memories: {e}")
            return []

    async def generate_embeddings(self, memories: List[RAGMemory]) -> List[RAGMemory]:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð² Ð´Ð»Ñ Ð²Ð¾ÑÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ð¹"""
        try:
            for memory in memories:
                if memory.embedding is None:
                    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ DeepSeek Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²
                    embedding = await self._get_embedding(memory.content)
                    memory.embedding = embedding

            return memories
        except Exception as e:
            self.logger.error(f"Error generating embeddings: {e}")
            return memories

    async def _get_embedding(self, text: str) -> List[float]:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³ Ñ‚ÐµÐºÑÑ‚Ð° Ñ‡ÐµÑ€ÐµÐ· DeepSeek"""
        # Ð”Ð»Ñ DeepSeek Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¸Ñ… API Ð´Ð»Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²
        # Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑƒÐ¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ - Ð¿Ð¾Ð·Ð¶Ðµ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ embeddings API
        prompt = f"""
        ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐ¹ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð² Ñ‡Ð¸ÑÐ»Ð¾Ð²Ð¾Ð¹ Ð²ÐµÐºÑ‚Ð¾Ñ€ Ð´Ð»Ñ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°.
        Ð¢ÐµÐºÑÑ‚: "{text}"

        Ð’ÐµÑ€Ð½Ð¸ Ð¢ÐžÐ›Ð¬ÐšÐž JSON Ñ Ð¼Ð°ÑÑÐ¸Ð²Ð¾Ð¼ Ð¸Ð· 384 Ñ‡Ð¸ÑÐµÐ»:
        {{"embedding": [Ñ‡Ð¸ÑÐ»Ð¾1, Ñ‡Ð¸ÑÐ»Ð¾2, ...]}}
        """

        try:
            messages = [{"role": "user", "content": prompt}]
            response = await self.ai_client.generate_response(messages, max_tokens=500)

            # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¸ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¿ÑÐµÐ²Ð´Ð¾-ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³
            # Ð’ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð½ÑƒÐ¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒä¸“é—¨çš„ embeddings API
            return self._generate_simple_embedding(text)

        except Exception as e:
            self.logger.error(f"Error getting embedding: {e}")
            return self._generate_simple_embedding(text)

    def _generate_simple_embedding(self, text: str) -> List[float]:
        """Ð£Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð° (Ð·Ð°Ð³Ð»ÑƒÑˆÐºÐ°)"""
        # Ð’ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ embeddings API DeepSeek
        import hashlib
        import struct

        # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð´ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ "ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³" Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ…ÐµÑˆÐ° Ñ‚ÐµÐºÑÑ‚Ð°
        hash_obj = hashlib.md5(text.encode())
        hash_bytes = hash_obj.digest()

        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð²ÐµÐºÑ‚Ð¾Ñ€ Ð¸Ð· 384 ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð² (ÐºÐ°Ðº Ð² Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…)
        embedding = []
        for i in range(384):
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ñ‡Ð°ÑÑ‚Ð¸ Ñ…ÐµÑˆÐ° Ð´Ð»Ñ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð°
            byte_idx = i % len(hash_bytes)
            value = (hash_bytes[byte_idx] / 255.0) * 2 - 1  # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ðº [-1, 1]
            embedding.append(round(value, 6))

        return embedding

    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ JSON Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ñ‚ LLM"""
        try:
            # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ñ… markdown Ð±Ð»Ð¾ÐºÐ¾Ð²
            cleaned_response = re.sub(r'```json\s*|\s*```', '', response).strip()
            return json.loads(cleaned_response)
        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to parse LLM response: {response}")
            return {"memories": []}

    def _format_conversation_context(self, context: List[Dict]) -> str:
        """Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð° Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°"""
        formatted = []
        for msg in context[-5:]:  # Ð‘ÐµÑ€ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 5 ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹
            role = "ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ" if msg['role'] == 'user' else "ÐÐ¹Ð½Ð°"
            formatted.append(f"{role}: {msg['content']}")

        return "\n".join(formatted)

    def prepare_memories_for_context(self, memories: List[RAGMemory], max_tokens: int = 500) -> str:
        """ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð²Ð¾ÑÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ð¹ Ð´Ð»Ñ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°"""
        if not memories:
            return ""

        context_parts = ["ðŸ’« Ð¯ Ð¿Ð¾Ð¼Ð½ÑŽ Ð¾ Ñ‚ÐµÐ±Ðµ:"]
        token_count = len("ðŸ’« Ð¯ Ð¿Ð¾Ð¼Ð½ÑŽ Ð¾ Ñ‚ÐµÐ±Ðµ:")

        for memory in sorted(memories, key=lambda x: x.importance_score, reverse=True):
            memory_text = f"â€¢ {memory.content}"
            memory_tokens = len(memory_text)

            if token_count + memory_tokens > max_tokens:
                break

            context_parts.append(memory_text)
            token_count += memory_tokens

        return "\n".join(context_parts)