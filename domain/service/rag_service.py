import json
import re
from typing import List, Optional, Dict, Any, Tuple
from domain.entity.rag_memory import RAGMemory, MemoryType
from domain.interfaces.ai_client import AIClientInterface
from infrastructure.monitoring.logging import StructuredLogger


class RAGService:
    """Ð¡ÐµÑ€Ð²Ð¸Ñ Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¸ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ"""

    def __init__(self, ai_client: AIClientInterface):
        self.ai_client = ai_client
        self.logger = StructuredLogger("rag_service")

    async def extract_memories_from_message(self, user_id: int, message: str) -> List[RAGMemory]:
        """Ð˜Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð²Ð°Ð¶Ð½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹ Ð˜Ð¡ÐšÐ›Ð®Ð§Ð˜Ð¢Ð•Ð›Ð¬ÐÐž Ð¸Ð· Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ"""

        # Ð£ÐŸÐ ÐžÐ©Ð•ÐÐÐ«Ð™ Ð˜ Ð‘ÐžÐ›Ð•Ð• Ð­Ð¤Ð¤Ð•ÐšÐ¢Ð˜Ð’ÐÐ«Ð™ ÐŸÐ ÐžÐœÐŸÐ¢
        system_prompt = """
    Ð¢Ñ‹ â€” Ñ‚Ð¾Ñ‡Ð½Ñ‹Ð¹ Ð¸ Ð±Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð½Ð¾Ð²Ñ‹Ñ… Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ðµ.

    ÐŸÐ ÐÐ’Ð˜Ð›Ð:
    1. ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð¢ÐžÐ›Ð¬ÐšÐž Ñ‚ÐµÐºÑƒÑ‰ÐµÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ
    2. Ð˜Ð·Ð²Ð»ÐµÐºÐ°Ð¹ Ð¢ÐžÐ›Ð¬ÐšÐž Ð½Ð¾Ð²Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹ (Ð½Ðµ Ñ‚Ðµ, Ñ‡Ñ‚Ð¾ ÑƒÐ¶Ðµ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°Ð»Ð¸ÑÑŒ)
    3. Ð˜Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÐ¹ Ð²ÑÑ‘, Ñ‡Ñ‚Ð¾ ÐºÐ°ÑÐ°ÐµÑ‚ÑÑ Ð±Ð¾Ñ‚Ð° (ÐÐ¹Ð½Ñ‹)
    4. Ð˜Ð·Ð²Ð»ÐµÐºÐ°Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð·Ð½Ð°Ñ‡Ð¸Ð¼Ñ‹Ðµ, Ð´Ð¾Ð»Ð³Ð¾ÑÑ€Ð¾Ñ‡Ð½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹

    Ð˜Ð—Ð’Ð›Ð•ÐšÐÐ™ Ð¢Ð˜ÐŸÐ« Ð¤ÐÐšÐ¢ÐžÐ’:
    - Ð›Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ð¸ Ð²ÐºÑƒÑÑ‹
    - Ð’Ð°Ð¶Ð½Ñ‹Ðµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ (Ð´Ð°Ñ‚Ñ‹, Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ñ)
    - Ð›Ð¸Ñ‡Ð½Ñ‹Ðµ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð¸ Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡ÐºÐ¸
    - Ð”Ð¾Ð»Ð³Ð¾ÑÑ€Ð¾Ñ‡Ð½Ñ‹Ðµ Ð¿Ð»Ð°Ð½Ñ‹ Ð¸ Ñ†ÐµÐ»Ð¸
    - Ð—Ð½Ð°Ñ‡Ð¸Ð¼Ñ‹Ðµ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ñ
    - Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð½Ð°Ð²Ñ‹ÐºÐ¸ Ð¸ ÑƒÐ¼ÐµÐ½Ð¸Ñ

    # ÐÐ• Ð˜Ð—Ð’Ð›Ð•ÐšÐÐ™:
    # - ÐŸÐ¾Ð²ÑÐµÐ´Ð½ÐµÐ²Ð½Ñ‹Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ (ÑÐµÐ³Ð¾Ð´Ð½Ñ Ð¿Ð¾ÑˆÐµÐ» Ð² Ð¼Ð°Ð³Ð°Ð·Ð¸Ð½)
    # - Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ ÑÐ¼Ð¾Ñ†Ð¸Ð¸ (ÑÐµÐ³Ð¾Ð´Ð½Ñ Ð³Ñ€ÑƒÑÑ‚Ð½Ð¾/Ð²ÐµÑÐµÐ»Ð¾)
    # - ÐžÐ±Ñ‰Ð¸Ðµ Ñ€Ð°Ð·Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ñ Ð±ÐµÐ· ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð¸ÐºÐ¸
    # - ÐŸÐ¾Ð²ÐµÑ€Ñ…Ð½Ð¾ÑÑ‚Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¸ Ð¾ Ð¿Ð¾Ð³Ð¾Ð´Ðµ Ð¸ Ñ‚.Ð´.
    
    Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð° - JSON:
    {
        "memories": [
            {
                "type": "fact|preference|event|personal_detail",
                "content": "ÐšÑ€Ð°Ñ‚ÐºÐ°Ñ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ° Ñ„Ð°ÐºÑ‚Ð°",
                "importance": 0.8,
                "reason": "ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ ÑÑ‚Ð¾Ñ‚ Ñ„Ð°ÐºÑ‚ Ð²Ð°Ð¶ÐµÐ½"
            }
        ]
    }

    Ð•ÑÐ»Ð¸ Ð² ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¸ Ð½ÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ñ… Ñ„Ð°ÐºÑ‚Ð¾Ð² - Ð²ÐµÑ€Ð½Ð¸ {"memories": []}
    """

        user_prompt = f"""
    Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ: "{message}"

    Ð˜Ð·Ð²Ð»ÐµÐºÐ¸ Ð¢ÐžÐ›Ð¬ÐšÐž Ð½Ð¾Ð²Ñ‹Ðµ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹ Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ðµ Ð¸Ð· ÑÑ‚Ð¾Ð³Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ.
    """

        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            response = await self.ai_client.generate_response(
                messages,
                max_tokens=500,  # Ð£Ð¼ÐµÐ½ÑŒÑˆÐ°ÐµÐ¼, Ñ‚.Ðº. Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¾Ð´Ð½Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ
                temperature=0.1
            )

            memories_data = self._parse_llm_response(response)

            memories = []
            for mem_data in memories_data.get('memories', []):
                content = mem_data['content'].lower()

                # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ñ„Ð°ÐºÑ‚Ñ‹, ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ðµ Ñ Ð±Ð¾Ñ‚Ð¾Ð¼
                bot_keywords = ['Ð°Ð¹Ð½Ð°', 'Ð±Ð¾Ñ‚', 'Ñ‚Ñ‹ ', 'Ñ‚ÐµÐ±Ðµ', 'Ñ‚Ð²Ð¾Ð¹', 'Ñ‚Ð²Ð¾Ñ', 'Ñ‚Ð²Ð¾Ñ‘', 'Ñƒ Ñ‚ÐµÐ±Ñ', 'Ñ‚ÐµÐ±Ñ']
                if any(keyword in content for keyword in bot_keywords):
                    self.logger.debug(f"Filtered bot-related memory: {mem_data['content']}")
                    continue

                memory = RAGMemory(
                    user_id=user_id,
                    memory_type=MemoryType(mem_data['type']),
                    content=mem_data['content'],
                    source_message=message,
                    importance_score=mem_data['importance'],
                    metadata={'extracted_from': 'current_message_only'}
                )
                memories.append(memory)

            self.logger.info(
                f"Extracted {len(memories)} memories from current message",
                extra={'user_id': user_id, 'message_length': len(message), 'memories_count': len(memories)}
            )

            return memories

        except Exception as e:
            self.logger.error(f"Error extracting memories: {e}")
            return []

    async def generate_embeddings(self, memories: List[RAGMemory]) -> List[RAGMemory]:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð² Ð´Ð»Ñ Ð²Ð¾ÑÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ð¹"""
        try:
            for memory in memories:
                if memory.embedding is None:
                    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ DeepSeek Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²
                    embedding = await self._get_embedding(memory.content)
                    memory.embedding = embedding

            return memories
        except Exception as e:
            self.logger.error(f"Error generating embeddings: {e}")
            return memories

    async def _get_embedding(self, text: str) -> List[float]:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³ Ñ‚ÐµÐºÑÑ‚Ð° Ñ‡ÐµÑ€ÐµÐ· DeepSeek"""
        # Ð”Ð»Ñ DeepSeek Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¸Ñ… API Ð´Ð»Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²
        # Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑƒÐ¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ - Ð¿Ð¾Ð·Ð¶Ðµ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ embeddings API
        prompt = f"""
        ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐ¹ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð² Ñ‡Ð¸ÑÐ»Ð¾Ð²Ð¾Ð¹ Ð²ÐµÐºÑ‚Ð¾Ñ€ Ð´Ð»Ñ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°.
        Ð¢ÐµÐºÑÑ‚: "{text}"

        Ð’ÐµÑ€Ð½Ð¸ Ð¢ÐžÐ›Ð¬ÐšÐž JSON Ñ Ð¼Ð°ÑÑÐ¸Ð²Ð¾Ð¼ Ð¸Ð· 384 Ñ‡Ð¸ÑÐµÐ»:
        {{"embedding": [Ñ‡Ð¸ÑÐ»Ð¾1, Ñ‡Ð¸ÑÐ»Ð¾2, ...]}}
        """

        try:
            messages = [{"role": "user", "content": prompt}]
            response = await self.ai_client.generate_response(messages, max_tokens=500)

            # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¸ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¿ÑÐµÐ²Ð´Ð¾-ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³
            # Ð’ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð½ÑƒÐ¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒä¸“é—¨çš„ embeddings API
            return self._generate_simple_embedding(text)

        except Exception as e:
            self.logger.error(f"Error getting embedding: {e}")
            return self._generate_simple_embedding(text)

    def _generate_simple_embedding(self, text: str) -> List[float]:
        """Ð£Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð° (Ð·Ð°Ð³Ð»ÑƒÑˆÐºÐ°)"""
        # Ð’ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ embeddings API DeepSeek
        import hashlib
        import struct

        # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð´ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ "ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³" Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ…ÐµÑˆÐ° Ñ‚ÐµÐºÑÑ‚Ð°
        hash_obj = hashlib.md5(text.encode())
        hash_bytes = hash_obj.digest()

        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð²ÐµÐºÑ‚Ð¾Ñ€ Ð¸Ð· 384 ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð² (ÐºÐ°Ðº Ð² Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…)
        embedding = []
        for i in range(384):
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ñ‡Ð°ÑÑ‚Ð¸ Ñ…ÐµÑˆÐ° Ð´Ð»Ñ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð°
            byte_idx = i % len(hash_bytes)
            value = (hash_bytes[byte_idx] / 255.0) * 2 - 1  # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ðº [-1, 1]
            embedding.append(round(value, 6))

        return embedding

    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ JSON Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ñ‚ LLM"""
        try:
            # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ñ… markdown Ð±Ð»Ð¾ÐºÐ¾Ð²
            cleaned_response = re.sub(r'```json\s*|\s*```', '', response).strip()
            return json.loads(cleaned_response)
        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to parse LLM response: {response}")
            return {"memories": []}

    def _format_conversation_context(self, context: List[Dict]) -> str:
        """Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð° Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°"""
        formatted = []
        for msg in context[-5:]:  # Ð‘ÐµÑ€ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 5 ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹
            role = "ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ" if msg['role'] == 'user' else "ÐÐ¹Ð½Ð°"
            formatted.append(f"{role}: {msg['content']}")

        return "\n".join(formatted)

    def prepare_memories_for_context(self, memories: List[RAGMemory], max_tokens: int = 500) -> str:
        """ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð²Ð¾ÑÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ð¹ Ð´Ð»Ñ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°"""
        if not memories:
            return ""

        context_parts = ["ðŸ’« Ð¯ Ð¿Ð¾Ð¼Ð½ÑŽ Ð¾ Ñ‚ÐµÐ±Ðµ:"]
        token_count = len("ðŸ’« Ð¯ Ð¿Ð¾Ð¼Ð½ÑŽ Ð¾ Ñ‚ÐµÐ±Ðµ:")

        for memory in sorted(memories, key=lambda x: x.importance_score, reverse=True):
            memory_text = f"â€¢ {memory.content}"
            memory_tokens = len(memory_text)

            if token_count + memory_tokens > max_tokens:
                break

            context_parts.append(memory_text)
            token_count += memory_tokens

        return "\n".join(context_parts)